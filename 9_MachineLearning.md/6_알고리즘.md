# 알고리즘
> 회귀(Regression)
> 결정트리(DecisionTree)
> K최근접이웃모델(K-Nearest Neighbors : KNN)

## 회귀

## 결정트리
- 이론
    - 스무고개와 같이 특정 질문들을 통해 정답을 찾아가는 모델
    - 최상단의 뿌리마디에서 마지막 끝 마디까지 아래 방향으로 성장
    - 최선의 분할은 데이터가 균일해지도록(순도가 높아짐, 불순도가 낮아짐) 분리기준을 잡습니다.
    - 불순도값에 의해 분리됨.
- 분리기준
    - 불순도 : 다른 데이터가 섞여 있는 정도
    - 지니불순도가 가장 낮은값을 갖는 기준으로 결정트리를 성장시킴.
    - 정보이득이 높은 값을 갖는 기준으로 결정트리를 성장시킴.
    - 즉, 불순한 것이 사라지도록 또는 정보가 점점 순수해져 분명하게 얻는 정보가 많아지도록 분리 기준을 잡습니다. 
- 장점
    - 해석이 직관적이고 쉽다.
    - 피처의 스케일링이나 정규화 등의 사전 가공 영향도가 크지 않습니다.
- 단점
    - 과적합으로 알고리즘 성능이 떨어짐
    - 이를 극복하기 위해서 트리의 크기나 리프노드의 샘플 수 등 파라미터들을 제한하는 튜닝이 필요함.
```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

from sklearn.tree import DecisionTreeClassifier
from sklearn.tree import plot_tree

import warnings
warnings.filterwarnings('ignore')

# 데이터 로드
iris_data = load_iris()

# 데이터 분석
# 데이터 전처리
x_train , x_test , y_train , y_test = train_test_split(iris_data.data, iris_data.target, test_size=0.2,  random_state=11)

# 모델 생성
1. model = DecisionTreeClassifier(random_state=156,max_depth=3) # 트리성장 깊이 조절 max_depth : 하이퍼파라미터 -> 머신러닝, 딥러닝에서 학스을 할 때 개발자가 학습 정도를 조절하는데 사용하는 파라미터
2. model = DecisionTreeClassifier(random_state=156,min_samples_leaf=5) # 리프노드의 최소 샘플 수 (리프노드가 5개일때까지 성장하라 => value = [0,3,5], [0,0,5])
3. model = DecisionTreeClassifier(random_state=111,min_samples_leaf=2,max_depth=4) # 리프노드 & 트리성장 깊이 조절 중 어느것이 우선?

# 하이퍼파라미터 중에는 기능적으로 상충 및 중복기능이 있을 수 있는데, 그 중 우선권을 가지는 파라미터가 있다.
# 여러개의 하이퍼파라미터를 사용할 때, 중복의미를 가지지는 않는지 확인 하자

# 모델 학습
model.fit(x_train , y_train)

# 모델 검증
print(model.score(x_train, y_train))
print(model.score(x_test, y_test)) 


# 모델 예측
plt.figure(figsize=(15,10))
plot_tree(model,filled=True, feature_names=iris_data.feature_names)

plt.show()
```

## K-Nearest Neighbors : K최근접 이웃 모델
- 분류하고자 하는 특성과 가장 가까운 k개(default 5개)를 찾음
- 그 중에 가장 많은 범주로 예측(다수결 투표)
- K(주변의갯수)라는 값 조절 적잘한 모델 찾음

```python
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier

# 데이터 로드
x_data = np.array([
    [2, 1],
    [3, 2],
    [3, 4],
    [5, 5],
    [7, 5],
    [2, 5],
    [8, 9],
    [9, 10],
    [6, 12],
    [9, 2],
    [6, 10],
    [2, 4]
])
y_data = np.array([0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0])

labels = ['fail', 'pass']

# 데이터 분석
pass

# 데이터 전처리
x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, test_size=0.3, random_state=777, stratify=y_data)

# 모델 생성
model = KNeighborsClassifier(n_neighbors=7) #하이퍼파라미터 튜닝 => 파라미터 모를 때 : 빈 셀에 KNeighborsClassifier?

# 모델 학습
result = model.fit(x_train, y_train)

# 모델 검증
print(model.score(x_train, y_train)) #

print(model.score(x_test, y_test)) #0.75

# 모델 예측
x_test = np.array([
    [4, 6]
])

y_predict = model.predict(x_test)
label = labels[y_predict[0]]
y_predict = model.predict_proba(x_test)
confidence = y_predict[0][y_predict[0].argmax()]

print(label, confidence) #

```



